{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQ29r70fF2ryiC07V3W9ig"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"-5mzKQGE4wkp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720018242308,"user_tz":-120,"elapsed":53607,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}},"outputId":"56005c46-f865-4bf1-fb0f-5271491ae5f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Run below commands in google colab\n","# install Java8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","# download spark3.0.0\n","#!wget -q http://apache.osuosl.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz\n","\n","#Per velocizzare il tutto si potrebbe eseguire il download del file a parte e poi quando bisogna usarlo, fare mount del gdrive e unzip solamente.\n","\n","# unzip it\n","!tar xf /content/drive/MyDrive/spark-3.4.2-bin-hadoop3.tgz\n","# install findspark\n","!pip install -q findspark\n","!pip install fuzzywuzzy"],"metadata":{"id":"8lZbsS7r5Laf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720018971942,"user_tz":-120,"elapsed":25142,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}},"outputId":"c0a93fa1-170a-4522-ffdf-4bb5c92849b8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fuzzywuzzy\n","  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n","Installing collected packages: fuzzywuzzy\n","Successfully installed fuzzywuzzy-0.18.0\n"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.2-bin-hadoop3\""],"metadata":{"id":"nEYpvyz-5NSV","executionInfo":{"status":"ok","timestamp":1720018275821,"user_tz":-120,"elapsed":6,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import findspark\n","findspark.init()"],"metadata":{"id":"Ycdkt3sk5POE","executionInfo":{"status":"ok","timestamp":1720018275821,"user_tz":-120,"elapsed":4,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Verify the Spark version running on the virtual cluster\n","from pyspark.context import SparkContext\n","sc = SparkContext.getOrCreate()\n","\n","assert  \"3.\" in sc.version, \"Verify that the cluster Spark's version is 3.x\""],"metadata":{"id":"oG_bAqI-5RxM","executionInfo":{"status":"ok","timestamp":1720018286373,"user_tz":-120,"elapsed":10556,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, collect_list, explode, collect_list, udf\n","from pyspark.sql.types import StringType\n","import pandas as pd\n","from fuzzywuzzy import process, fuzz"],"metadata":{"id":"Z9-wOcy95YLL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720018981509,"user_tz":-120,"elapsed":295,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}},"outputId":"1c12fe91-298f-452b-e454-9be6ab11ec0b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n","  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"]}]},{"cell_type":"code","source":["spark = SparkSession(sc)\n","print(spark)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aPuWWRK6563z","executionInfo":{"status":"ok","timestamp":1720018287348,"user_tz":-120,"elapsed":977,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}},"outputId":"3cc1048c-4f0c-44a7-b55c-6f07e4ff9068"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["<pyspark.sql.session.SparkSession object at 0x7cf14a75a290>\n"]}]},{"cell_type":"code","source":["# Funzione utile a verificare se il nome di un'entità di tipo Person è contenuto all'interno di un'altra entità\n","def create_name_mapping(names):\n","    name_mapping = {}\n","    for name in names:\n","        for other_name in names:\n","            if (name != other_name and name.lower() in other_name.lower()) or (name != other_name and name.lower() == other_name.lower()):\n","                name_mapping[name] = other_name\n","                break\n","        if name not in name_mapping:\n","            name_mapping[name] = name\n","    return name_mapping\n","\n","# Funzione per trovare i nomi completi degli acronimi\n","def find_full_name(name, mapping_dict):\n","    if name is not None:\n","        return mapping_dict.get(name.lower(), name)\n","    return name\n","\n","\n","@udf(StringType())\n","def replace_acronyms(name):\n","    return mapping_dict.get(name.lower(), name)\n","\n","def find_canonical_name(name, names_list, threshold=85):\n","    match = process.extractOne(name, names_list, scorer=fuzz.token_sort_ratio)\n","    if match and match[1] >= threshold:\n","        return match[0]\n","    return name\n","\n","@udf(StringType())\n","def map_names(name):\n","    return name_map.get(name, name)"],"metadata":{"id":"io_oWq0HFD2o","executionInfo":{"status":"ok","timestamp":1720025724260,"user_tz":-120,"elapsed":276,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["# Liste di verifica\n","entities_list = [\"Person\", \"Organization\", \"Location\", \"Politician\", \"Party\", \"Event\", \"Agreement\"]\n","relationship_list = [\"leader_of\", \"is_from\", \"part_of\", \"located_in\", \"member_of\", \"supports\", \"president_of\", \"opposition\", \"colleague\"]\n","\n","# Carica i file CSV in DataFrame Spark\n","df_entities = spark.read.csv(\"/content/drive/MyDrive/FinalProject/entities.csv\", header=True)\n","df_relations = spark.read.csv(\"/content/drive/MyDrive/FinalProject/relations.csv\", header=True)\n","df_acronimi = spark.read.csv(\"/content/drive/MyDrive/FinalProject/check_entities.csv\", header=True)\n","\n","# Elimina duplicati e filtra per tipi consentiti\n","df_entities = df_entities.withColumn('Entity_lower', lower(col('Name')))\\\n","                          .dropDuplicates(['Entity_lower', 'Type'])\\\n","                          .filter(col('Type').isin(entities_list))\\\n","                          .drop('Entity_lower')\n","\n","df_relations = df_relations.dropDuplicates(['Relation', 'Source', 'Target'])\\\n","                           .filter(col('Relation').isin(relationship_list))"],"metadata":{"id":"OpmXxBrTFJ2U","executionInfo":{"status":"ok","timestamp":1720022993767,"user_tz":-120,"elapsed":975,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","execution_count":67,"metadata":{"id":"wxVJ_8PuEWR4","executionInfo":{"status":"ok","timestamp":1720023844106,"user_tz":-120,"elapsed":1923,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"outputs":[],"source":["mapping_dict = dict(df_acronimi.rdd.map(lambda row: (row['Old_Name'].lower(), row['New_Name'])).collect())"]},{"cell_type":"code","source":["df_entities = df_entities.withColumn(\"Name\", replace_acronyms(col(\"Name\")))\n","df_relations = df_relations.withColumn(\"Source\", replace_acronyms(col(\"Source\")))\n","df_relations = df_relations.withColumn(\"Target\", replace_acronyms(col(\"Target\")))"],"metadata":{"id":"xphqg5TmR-fU","executionInfo":{"status":"ok","timestamp":1720023954815,"user_tz":-120,"elapsed":294,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["unique_names = df_entities.select(\"Name\").distinct().rdd.flatMap(lambda x: x).collect()\n","name_map = {name: find_canonical_name(name, unique_names) for name in unique_names}"],"metadata":{"id":"kvQN3Eara5pk","executionInfo":{"status":"ok","timestamp":1720024903031,"user_tz":-120,"elapsed":837540,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["df_person = df_entities.select(\"Name\").filter(col(\"Type\") == \"Person\").dropDuplicates()\n","names = [row[\"Name\"] for row in df_person.collect()]\n","name_map_person = create_name_mapping(names)"],"metadata":{"id":"oz1rHY63dSHw","executionInfo":{"status":"ok","timestamp":1720025731391,"user_tz":-120,"elapsed":2795,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["name_map.update(name_map_person)"],"metadata":{"id":"roz17ZU-heVV","executionInfo":{"status":"ok","timestamp":1720025847824,"user_tz":-120,"elapsed":368,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["df_entities = df_entities.withColumn(\"Name\", map_names(col(\"Name\")))\n","df_relations = df_relations.withColumn(\"Source\", map_names(col(\"Source\")))\n","df_relations = df_relations.withColumn(\"Target\", map_names(col(\"Target\")))"],"metadata":{"id":"lLvN--VGblp7","executionInfo":{"status":"ok","timestamp":1720025918093,"user_tz":-120,"elapsed":299,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["df_unique_entities = df_entities.dropDuplicates()\n","df_unique_relations = df_relations.dropDuplicates()"],"metadata":{"id":"Sihd4AQvbpSX","executionInfo":{"status":"ok","timestamp":1720027804793,"user_tz":-120,"elapsed":287,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}}},"execution_count":149,"outputs":[]},{"cell_type":"code","source":["df_unique_entities = df_unique_entities.toPandas()\n","df_unique_relations = df_unique_relations.toPandas()\n","\n","df_unique_entities.to_csv(\"/content/drive/MyDrive/FinalProject/unique_entities.csv\", index=False)\n","df_unique_relations.to_csv(\"/content/drive/MyDrive/FinalProject/unique_relations.csv\", index=False)\n","\n","print(\"File con righe uniche (nomi simili gestiti) salvati come unique_entities.csv e unique_relations.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"mEHO8Pxbb5BF","executionInfo":{"status":"error","timestamp":1720027806851,"user_tz":-120,"elapsed":1142,"user":{"displayName":"Giuseppe BigData","userId":"00128238439538601404"}},"outputId":"9d82b2b1-2c0b-4e52-f1a4-8ba7580819b5"},"execution_count":150,"outputs":[{"output_type":"error","ename":"PythonException","evalue":"\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-68-1169a7b7f0a8>\", line 29, in replace_acronyms\nAttributeError: 'NoneType' object has no attribute 'lower'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-150-eb59f312c2ae>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_unique_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_unique_entities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_unique_relations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_unique_relations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_unique_entities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/FinalProject/unique_entities.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_unique_relations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/FinalProject/unique_relations.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.4.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.4.2-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.4.2-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.4.2-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-68-1169a7b7f0a8>\", line 29, in replace_acronyms\nAttributeError: 'NoneType' object has no attribute 'lower'\n"]}]},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"ElwXsiR2b5nv"},"execution_count":null,"outputs":[]}]}